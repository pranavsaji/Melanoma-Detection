{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDriIbfa5lwD"
   },
   "source": [
    "Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvR7ppk77v31"
   },
   "source": [
    "### Importing Skin Cancer Data\n",
    "#### To do: Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfcpIXQZN2Rh"
   },
   "source": [
    "### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9k0ORabJzwW3",
    "outputId": "dffb352d-82a0-49a3-8c23-2266bfeb5a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\pranav\\anaconda3\\lib\\site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install  keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WC8xCQuELWms"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYpVPmT5z7AP",
    "outputId": "c93398fb-8227-4ee1-91f4-7a14373eba1a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## If you are using the data by mounting the google drive, use the following :\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "## If you are using the data by mounting the google drive, use the following :\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpUsRQwOOL72"
   },
   "source": [
    "The dataset used for this project contains roughly 2357 photos of various skin cancers. Each of the train and test subdirectories in the dataset has 9 subdirectories. The photos of the nine different forms of skin cancer are present in each of  the nine subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D57L-ovIKtI4"
   },
   "outputs": [],
   "source": [
    "# Defining the path for train and test images\n",
    "\n",
    "data_directory_train = pathlib.Path(\"/content/gdrive/MyDrive/PGD /CNN/Train\")\n",
    "data_directory_test = pathlib.Path('/content/gdrive/MyDrive/PGD /CNN/Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqksN1w5Fu-N",
    "outputId": "9e8d5b49-d085-4fbe-ecce-8c41f597a26a"
   },
   "outputs": [],
   "source": [
    "image_count_train = len(list(data_directory_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "image_count_test = len(list(data_directory_test.glob('*/*.jpg')))\n",
    "print(image_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8HkfW3jPJun"
   },
   "source": [
    "### Load using keras.preprocessing\n",
    "\n",
    "Let's use the useful image_dataset_from_directory utility to load these images from disc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDBKZG3jPcMc"
   },
   "source": [
    "### Create a dataset\n",
    "\n",
    "Define some parameters for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLfcXcZ9LjGv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1BWmDzr7w-5",
    "outputId": "89f4ca6e-ea1d-41cb-b197-782fea2a4a16"
   },
   "outputs": [],
   "source": [
    "## Note your train dataset here\n",
    "## Note use seed=143 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_directory_train,batch_size=batch_size,image_size=(img_height,img_width),label_mode='categorical',\n",
    "                                                              seed=143,subset=\"training\",validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYch6-SR-i2g",
    "outputId": "30b36daf-dffc-4d06-dc81-63642b263719"
   },
   "outputs": [],
   "source": [
    "## Note your validation dataset here\n",
    "## Note use seed=143 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
    "val_ds =tf.keras.preprocessing.image_dataset_from_directory(data_directory_train,batch_size=batch_size,image_size=(img_height,img_width),label_mode='categorical',\n",
    "                                                              seed=143,subset=\"validation\",validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bk0RV7G7-nad",
    "outputId": "b40a4da1-d5ff-4a47-c5f5-58bf7a9158c5"
   },
   "outputs": [],
   "source": [
    "# List out all the classes of skin cancer and store them in a list. \n",
    "# You can find the class names in the class_names attribute on these datasets. \n",
    "# These correspond to the directory names in alphabetical order.\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbsm5oYiQH_b"
   },
   "source": [
    "### Visualize the data\n",
    "#### Create a code to visualize one instance of all the nine classes present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "tKILZ48I-q1k",
    "outputId": "c7c80591-310b-4329-fb3e-e9c71ffbbd0c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "#Dictionary to store the path of image as per the class\n",
    "files_path_dict = {}\n",
    "\n",
    "for c in class_names:\n",
    "    files_path_dict[c] = list(map(lambda x:str(data_directory_train)+'/'+c+'/'+x,os.listdir(str(data_directory_train)+'/'+c)))\n",
    "    \n",
    "#Visualize image \n",
    "plt.figure(figsize=(15,15))\n",
    "index = 0\n",
    "for c in class_names:\n",
    "    path_list = files_path_dict[c][:1]\n",
    "    index += 1\n",
    "    plt.subplot(3,3,index)\n",
    "    plt.imshow(load_img(path_list[0],target_size=(img_height,img_width)))\n",
    "    plt.title(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cAZPYaeQjQy"
   },
   "source": [
    "A tensor with the shape \"(32, 180, 180, 3)\" is the \"image batch.\" This collection of 32 pictures shows the shape \"180x180x3\" (the last dimension refers to colour channels RGB). The \"label batch\" is a tensor with the shape \"(32,)\" and contains labels for each of the 32 photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzVXBHiyQ7_I"
   },
   "source": [
    "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n",
    "\n",
    "`Dataset.prefetch()` overlaps data preprocessing and model execution while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wZlKRBEGNtU"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JEAF6-sRyz8"
   },
   "source": [
    "### Create the model\n",
    "#### Create a CNN model, which can accurately detect 9 classes present in the dataset. Use ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ync9xoW7GZgn"
   },
   "outputs": [],
   "source": [
    "input_shape = (img_height,img_width,3)\n",
    "\n",
    "model = Sequential()    #Sequential allows you to create models layer-by-layer  \n",
    "\n",
    "#First Convolution Layer\n",
    "model.add(layers.experimental.preprocessing.Rescaling(1./255,input_shape=input_shape))\n",
    "model.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Second Convolution Layer\n",
    "model.add(layers.Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Third Convolution Layer\n",
    "model.add(layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(layers.Flatten())   #Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension.\n",
    "\n",
    "#Dense Layer\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "\n",
    "#Dense Layer\n",
    "model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "#Dense Layer with softmax activation function.\n",
    "#Softmax is an activation function that scales numbers/logits into probabilities.\n",
    "model.add(layers.Dense(len(class_names),activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDKzJmHwSCtt"
   },
   "source": [
    "### Compile the model\n",
    "Choose an appropirate optimiser and loss function for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB8wKtiPGe1j"
   },
   "outputs": [],
   "source": [
    "### Todo, choose an appropirate optimiser and loss function\n",
    "model.compile(optimizer='Adam',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZGWN4MZGhtJ",
    "outputId": "305100e3-bada-41aa-84ff-fa3ebbb4cb03"
   },
   "outputs": [],
   "source": [
    "# View the summary of all layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljD_83rwSl5O"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kkfw2rJXGlYC",
    "outputId": "aa7afa51-04b3-42b0-8173-02596f3633f4"
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3679V8OShSE"
   },
   "source": [
    "### Visualizing training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "R1xkgk5nGubz",
    "outputId": "576c8087-1b5c-4564-f5d6-b6d3e84e8988"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22hljAl6GykA"
   },
   "outputs": [],
   "source": [
    "#Data augumentation strategy. \n",
    "\n",
    "rescale = tf.keras.Sequential([\n",
    "  #To rescale an input in the [0, 255] range to be in the [0, 1] range  \n",
    "  layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  #Randomly flip each image horizontally and vertically.\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    \n",
    "  #Randomly rotate each image.\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    \n",
    "  #Randomly zoom each image during training.\n",
    "  layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    \n",
    "  #Randomly translate each image during training.\n",
    "  layers.experimental.preprocessing.RandomTranslation(0.1, 0.1)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "XEjPWh8GG0C7",
    "outputId": "40a4c963-f8db-498b-ff31-79471d6f0b6a"
   },
   "outputs": [],
   "source": [
    "#Visualize the augmentation image\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):   \n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkCdQVRc6eqG"
   },
   "outputs": [],
   "source": [
    "## Model 2 Creation\n",
    "\n",
    "#Dropout layer: In order to avoid overfitting, the training process randomly sets input units to 0 with a frequency of rate. The sum of all inputs is maintained by scaling up non-zero inputs by 1/(1 - rate).\n",
    "\n",
    "\n",
    "## Your code goes here\n",
    "model2 = Sequential()                     #Sequential allows you to create models layer-by-layer  \n",
    "\n",
    "model2.add(data_augmentation)             #Augmentation layer\n",
    "model2.add(rescale)                       #Rescaling layer\n",
    "\n",
    "#First Convolution Layer\n",
    "model2.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Dropout layer with 25% Fraction of the input units to drop.\n",
    "model2.add(layers.Dropout(0.25))\n",
    "\n",
    "#Second Convolution Layer\n",
    "model2.add(layers.Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Dropout layer with 25% Fraction of the input units to drop.\n",
    "model2.add(layers.Dropout(0.25))\n",
    "\n",
    "#Third Convolution Layer\n",
    "model2.add(layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "model2.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension.\n",
    "model2.add(layers.Flatten())\n",
    "\n",
    "#Dense Layer\n",
    "model2.add(layers.Dense(512,activation='relu'))\n",
    "\n",
    "#Dense Layer\n",
    "model2.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "#Dropout layer with 50% Fraction of the input units to drop.\n",
    "model2.add(layers.Dropout(0.50))\n",
    "\n",
    "#Dense Layer with softmax activation function.\n",
    "#Softmax is an activation function that scales numbers/logits into probabilities.\n",
    "model2.add(layers.Dense(len(class_names),activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfUWFp96UIAN"
   },
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-7yTm8IG8zR"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='Adam',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC-D_RWOURp6"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcPfkUASHBf9",
    "outputId": "bccef3ab-2f63-475b-dfb8-94f388386fdb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "epochs =25\n",
    "history = model2.fit(train_ds,epochs=epochs,validation_data=val_ds,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhNOKtSyUYzC"
   },
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "vjN_F4QxHIsh",
    "outputId": "a3fee72e-c5f9-49dd-e42c-5e407a6617c2"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TdDi4u-VTkW"
   },
   "source": [
    "#### **Todo:** Find the distribution of classes in the training dataset.\n",
    "#### **Context:** Real-world datasets frequently exhibit class imbalance, with one class frequently having a disproportionately higher number of samples than the others. Unbalanced class composition may be detrimental to the quality of the final model. Therefore, it becomes crucial to look at the distribution of classes in the data as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "HAhwYgtTQRzq",
    "outputId": "1be57130-3087-4dd9-b5e2-fceb1c6b1c26"
   },
   "outputs": [],
   "source": [
    "\n",
    "def class_distribution_count(directory):\n",
    "    \n",
    "    #count number of image in each classes\n",
    "    count= []\n",
    "    for path in pathlib.Path(directory).iterdir():\n",
    "        if path.is_dir():\n",
    "            count.append(len([name for name in os.listdir(path)\n",
    "                               if os.path.isfile(os.path.join(path, name))]))\n",
    "    \n",
    "    #name of the classes\n",
    "    sub_directory = [name for name in os.listdir(directory)\n",
    "                    if os.path.isdir(os.path.join(directory, name))]\n",
    "    \n",
    "    #return dataframe with image count and class.\n",
    "    return pd.DataFrame(list(zip(sub_directory,count)),columns =['Class', 'No. of Image'])\n",
    "\n",
    "df = class_distribution_count(data_dir_train)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "zNeK-41KBRez",
    "outputId": "2d9b42fe-10de-4c2b-82a6-7b9428922c2b"
   },
   "outputs": [],
   "source": [
    "#Visualize the Number of image in each class.\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=\"No. of Image\", y=\"Class\", data=df,\n",
    "            label=\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb-stKyHPf8v"
   },
   "source": [
    "#### **Todo:** Rectify the class imbalance\n",
    "#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItAg4rU-SzJh",
    "outputId": "0c6f9383-fe64-4fff-f876-ef44c31efd97"
   },
   "outputs": [],
   "source": [
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZKzTe3zWL4O"
   },
   "source": [
    "To use `Augmentor`, the following general procedure is followed:\n",
    "\n",
    "1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n",
    "2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n",
    "3. Execute these operations by calling the `Pipelineâ€™s` `sample()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Egt9EHjR-Dd",
    "outputId": "ccb5203d-4c8b-4ce7-f567-c677b63c2618"
   },
   "outputs": [],
   "source": [
    "path_to_training_dataset=\"/content/gdrive/MyDrive/PGD /CNN/Train/\"\n",
    "import Augmentor\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcBIFZGbWuFa"
   },
   "source": [
    "Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxWcMqZhdRWz",
    "outputId": "6880d75e-6ab3-43cb-daeb-ad113afe2e90"
   },
   "outputs": [],
   "source": [
    "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
    "print(image_count_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ5KarKq4kWJ"
   },
   "source": [
    "### Lets see the distribution of augmented data after adding new images to the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tODrYIY2nxJ"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "path_list_new = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZvVdF7g3E1z"
   },
   "outputs": [],
   "source": [
    "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okcqVFAA2nxK"
   },
   "outputs": [],
   "source": [
    "dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njzBxTNT2nxK"
   },
   "outputs": [],
   "source": [
    "#dataframe that store path and label of the images generated by Augmentor\n",
    "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j45rmxd2nxK",
    "outputId": "8b1454f4-f32c-4bcc-8d12-19aadb3ba3f5"
   },
   "outputs": [],
   "source": [
    "df2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NirFBvGPmgI"
   },
   "source": [
    "To maintain some class balance, we have now added 500 photos to all the classes. To enhance the training process, we can add more images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EnspeMbRWNs"
   },
   "source": [
    "#### Train the model on the data created using Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFcj1XgndRWz"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0haOU11Ey8ey"
   },
   "source": [
    "#### Creating a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4ZY11judRWz",
    "outputId": "96be995a-6aa6-4fd7-b0da-f662c790af6c"
   },
   "outputs": [],
   "source": [
    "data_dir_train=\"/content/gdrive/MyDrive/PGD /CNN/Train/\"\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  seed=143,\n",
    "  validation_split = 0.25,\n",
    "  subset = \"training\",\n",
    "  image_size  = (img_height, img_width),label_mode='categorical', \n",
    "  batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwNJVDuBP5kf"
   },
   "source": [
    "#### Creating a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TX191d_3dRW0",
    "outputId": "70db5660-4e55-4631-ac99-daea613940e6"
   },
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  seed=143,\n",
    "  validation_split = 0.25,\n",
    "  subset =\"validation\",\n",
    "  image_size=(img_height, img_width),label_mode='categorical', \n",
    "  batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaoWeOEpVjqH"
   },
   "source": [
    "#### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch0MuKvFVr7O"
   },
   "outputs": [],
   "source": [
    "## your code goes here\n",
    "\n",
    "#Model\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(rescale)   #Rescaling Layer\n",
    "\n",
    "#First Convolution layer\n",
    "model3.add(layers.Conv2D(32,kernel_size=(2,2),activation='relu'))\n",
    "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model3.add(layers.Dropout(0.25))\n",
    "\n",
    "#Second Convolution Layer\n",
    "model3.add(layers.Conv2D(64,kernel_size=(2,2),activation='relu'))\n",
    "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model3.add(layers.Dropout(0.25))\n",
    "\n",
    "#Third Convolution Layer\n",
    "model3.add(layers.Conv2D(128,kernel_size=(2,2),activation='relu'))\n",
    "model3.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Flatten Layer\n",
    "model3.add(layers.Flatten())\n",
    "\n",
    "#Dense Layer\n",
    "model3.add(layers.Dense(512,activation='relu'))\n",
    "\n",
    "#Dropout layer\n",
    "model3.add(layers.Dropout(0.25))\n",
    "\n",
    "#Batch normalisation is a technique used to speed up and stabilise artificial neural networks by re-centering and re-scaling the inputs to the layers.\n",
    "model3.add(layers.BatchNormalization())\n",
    "\n",
    "#Dense Layer\n",
    "model3.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "#Dropout layer with 50% Fraction of the input units to drop.\n",
    "model3.add(layers.Dropout(0.50))\n",
    "\n",
    "#Batch normalization\n",
    "model3.add(layers.BatchNormalization())\n",
    "\n",
    "#Dense layer with Softmax activation function.\n",
    "model3.add(layers.Dense(len(class_names),activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu5N9LxkVx1B"
   },
   "source": [
    "#### Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H47GWmLbdRW1"
   },
   "outputs": [],
   "source": [
    "## your code goes here\n",
    "model3.compile(optimizer='Adam',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gS-Y1bJV7uy"
   },
   "source": [
    "####  Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcV6OdI4dRW1",
    "outputId": "0970e7ca-76a0-4edb-c4c7-ae0ebb3c373b"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "## Your code goes here, use 30 epochs.\n",
    "history = model3.fit(train_ds,epochs=epochs,validation_data=val_ds,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuvfCTsBWLMp"
   },
   "source": [
    "#### **Todo:**  Visualize the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "lCTXwfkTdRW1",
    "outputId": "8be09c41-8543-4c61-9afc-aaddea3306f7"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Starter_code_Assignment_CNN_Skin_Cancer (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
